{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os \n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global variables for running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_timesteps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A.US_D1.csv', 'AAL.US_D1.csv', 'AAPL.US_D1.csv', 'ABBV.US_D1.csv', 'ABNB.US_D1.csv', 'ABT.US_D1.csv', 'ACGL.US_D1.csv', 'ACN.US_D1.csv', 'ADBE.US_D1.csv', 'ADI.US_D1.csv', 'ADM.US_D1.csv', 'ADP.US_D1.csv', 'ADSK.US_D1.csv', 'AEE.US_D1.csv', 'AEP.US_D1.csv', 'AES.US_D1.csv', 'AFL.US_D1.csv', 'AIG.US_D1.csv', 'AIZ.US_D1.csv', 'AJG.US_D1.csv', 'AKAM.US_D1.csv', 'ALB.US_D1.csv', 'ALGN.US_D1.csv', 'ALL.US_D1.csv', 'ALLE.US_D1.csv', 'AMAT.US_D1.csv', 'AMCR.US_D1.csv', 'AMD.US_D1.csv', 'AME.US_D1.csv', 'AMGN.US_D1.csv', 'AMP.US_D1.csv', 'AMT.US_D1.csv', 'AMZN.US_D1.csv', 'ANET.US_D1.csv', 'ANSS.US_D1.csv', 'AON.US_D1.csv', 'AOS.US_D1.csv', 'APA.US_D1.csv', 'APD.US_D1.csv', 'APH.US_D1.csv', 'APTV.US_D1.csv', 'ARE.US_D1.csv', 'ASML.US_D1.csv', 'ATO.US_D1.csv', 'AVB.US_D1.csv', 'AVGO.US_D1.csv', 'AVY.US_D1.csv', 'AWK.US_D1.csv', 'AXON.US_D1.csv', 'AXP.US_D1.csv', 'AZN.US_D1.csv', 'AZO.US_D1.csv', 'BA.US_D1.csv', 'BAC.US_D1.csv', 'BALL.US_D1.csv', 'BAX.US_D1.csv', 'BBWI.US_D1.csv', 'BBY.US_D1.csv', 'BDX.US_D1.csv', 'BEN.US_D1.csv', 'BF.B.US_D1.csv', 'BG.US_D1.csv', 'BIIB.US_D1.csv', 'BIO.US_D1.csv', 'BK.US_D1.csv', 'BKNG.US_D1.csv', 'BKR.US_D1.csv', 'BLDR.US_D1.csv', 'BLK.US_D1.csv', 'BMY.US_D1.csv', 'BR.US_D1.csv', 'BRK.B.US_D1.csv', 'BRO.US_D1.csv', 'BSX.US_D1.csv', 'BWA.US_D1.csv', 'BX.US_D1.csv', 'BXP.US_D1.csv', 'C.US_D1.csv', 'CAG.US_D1.csv', 'CAH.US_D1.csv', 'CARR.US_D1.csv', 'CAT.US_D1.csv', 'CB.US_D1.csv', 'CBOE.US_D1.csv', 'CBRE.US_D1.csv', 'CCEP.US_D1.csv', 'CCI.US_D1.csv', 'CCL.US_D1.csv', 'CDAY.US_D1.csv', 'CDNS.US_D1.csv', 'CDW.US_D1.csv', 'CE.US_D1.csv', 'CEG.US_D1.csv', 'CF.US_D1.csv', 'CFG.US_D1.csv', 'CHD.US_D1.csv', 'CHRW.US_D1.csv', 'CHTR.US_D1.csv', 'CI.US_D1.csv', 'CINF.US_D1.csv', 'CL.US_D1.csv', 'CLX.US_D1.csv', 'CMA.US_D1.csv', 'CMCSA.US_D1.csv', 'CME.US_D1.csv', 'CMG.US_D1.csv', 'CMI.US_D1.csv', 'CMS.US_D1.csv', 'CNC.US_D1.csv', 'CNP.US_D1.csv', 'COF.US_D1.csv', 'COO.US_D1.csv', 'COP.US_D1.csv', 'COR.US_D1.csv', 'COST.US_D1.csv', 'CPB.US_D1.csv', 'CPRT.US_D1.csv', 'CPT.US_D1.csv', 'CRL.US_D1.csv', 'CRM.US_D1.csv', 'CRWD.US_D1.csv', 'CSCO.US_D1.csv', 'CSGP.US_D1.csv', 'CSX.US_D1.csv', 'CTAS.US_D1.csv', 'CTLT.US_D1.csv', 'CTRA.US_D1.csv', 'CTSH.US_D1.csv', 'CTVA.US_D1.csv', 'CVS.US_D1.csv', 'CVX.US_D1.csv', 'CZR.US_D1.csv', 'D.US_D1.csv', 'D1_Modified', 'DAL.US_D1.csv', 'DASH.US_D1.csv', 'DD.US_D1.csv', 'DDOG.US_D1.csv', 'DE.US_D1.csv', 'DFS.US_D1.csv', 'DG.US_D1.csv', 'DGX.US_D1.csv', 'DHI.US_D1.csv', 'DHR.US_D1.csv', 'DIS.US_D1.csv', 'DLR.US_D1.csv', 'DLTR.US_D1.csv', 'DOV.US_D1.csv', 'DOW.US_D1.csv', 'DPZ.US_D1.csv', 'DRI.US_D1.csv', 'DTE.US_D1.csv', 'DUK.US_D1.csv', 'DVA.US_D1.csv', 'DVN.US_D1.csv', 'DXCM.US_D1.csv', 'EA.US_D1.csv', 'EBAY.US_D1.csv', 'ECL.US_D1.csv', 'ED.US_D1.csv', 'EFX.US_D1.csv', 'EG.US_D1.csv', 'EIX.US_D1.csv', 'EL.US_D1.csv', 'ELV.US_D1.csv', 'EMN.US_D1.csv', 'EMR.US_D1.csv', 'ENPH.US_D1.csv', 'EOG.US_D1.csv', 'EPAM.US_D1.csv', 'EQIX.US_D1.csv', 'EQR.US_D1.csv', 'EQT.US_D1.csv', 'ES.US_D1.csv', 'ESS.US_D1.csv', 'ETN.US_D1.csv', 'ETR.US_D1.csv', 'ETSY.US_D1.csv', 'EVRG.US_D1.csv', 'EW.US_D1.csv', 'EXC.US_D1.csv', 'EXPD.US_D1.csv', 'EXPE.US_D1.csv', 'EXR.US_D1.csv', 'F.US_D1.csv', 'FANG.US_D1.csv', 'FAST.US_D1.csv', 'FCX.US_D1.csv', 'FDS.US_D1.csv', 'FDX.US_D1.csv', 'FE.US_D1.csv', 'FFIV.US_D1.csv', 'FI.US_D1.csv', 'FICO.US_D1.csv', 'FIS.US_D1.csv', 'FITB.US_D1.csv', 'FLT.US_D1.csv', 'FMC.US_D1.csv', 'FOX.US_D1.csv', 'FOXA.US_D1.csv', 'FRT.US_D1.csv', 'FSLR.US_D1.csv', 'FTNT.US_D1.csv', 'FTV.US_D1.csv', 'GD.US_D1.csv', 'GE.US_D1.csv', 'GEHC.US_D1.csv', 'GEN.US_D1.csv', 'GFS.US_D1.csv', 'GILD.US_D1.csv', 'GIS.US_D1.csv', 'GL.US_D1.csv', 'GLW.US_D1.csv', 'GM.US_D1.csv', 'GNRC.US_D1.csv', 'GOOG.US_D1.csv', 'GOOGL.US_D1.csv', 'GPC.US_D1.csv', 'GPN.US_D1.csv', 'GRMN.US_D1.csv', 'GS.US_D1.csv', 'GWW.US_D1.csv', 'HAL.US_D1.csv', 'HAS.US_D1.csv', 'HBAN.US_D1.csv', 'HCA.US_D1.csv', 'HD.US_D1.csv', 'HES.US_D1.csv', 'HIG.US_D1.csv', 'HII.US_D1.csv', 'HLT.US_D1.csv', 'HOLX.US_D1.csv', 'HON.US_D1.csv', 'HPE.US_D1.csv', 'HPQ.US_D1.csv', 'HRL.US_D1.csv', 'HSIC.US_D1.csv', 'HST.US_D1.csv', 'HSY.US_D1.csv', 'HUBB.US_D1.csv', 'HUM.US_D1.csv', 'HWM.US_D1.csv', 'IBM.US_D1.csv', 'ICE.US_D1.csv', 'IDXX.US_D1.csv', 'IEX.US_D1.csv', 'IFF.US_D1.csv', 'ILMN.US_D1.csv', 'INCY.US_D1.csv', 'INTC.US_D1.csv', 'INTU.US_D1.csv', 'INVH.US_D1.csv', 'IP.US_D1.csv', 'IPG.US_D1.csv', 'IQV.US_D1.csv', 'IR.US_D1.csv', 'IRM.US_D1.csv', 'ISRG.US_D1.csv', 'IT.US_D1.csv', 'ITW.US_D1.csv', 'IVZ.US_D1.csv', 'J.US_D1.csv', 'JBHT.US_D1.csv', 'JBL.US_D1.csv', 'JCI.US_D1.csv', 'JKHY.US_D1.csv', 'JNJ.US_D1.csv', 'JNPR.US_D1.csv', 'JPM.US_D1.csv', 'K.US_D1.csv', 'KDP.US_D1.csv', 'KEY.US_D1.csv', 'KEYS.US_D1.csv', 'KHC.US_D1.csv', 'KIM.US_D1.csv', 'KLAC.US_D1.csv', 'KMB.US_D1.csv', 'KMI.US_D1.csv', 'KMX.US_D1.csv', 'KO.US_D1.csv', 'KR.US_D1.csv', 'KVUE.US_D1.csv', 'L.US_D1.csv', 'LDOS.US_D1.csv', 'LEN.US_D1.csv', 'LH.US_D1.csv', 'LHX.US_D1.csv', 'LIN.US_D1.csv', 'LKQ.US_D1.csv', 'LLY.US_D1.csv', 'LMT.US_D1.csv', 'LNT.US_D1.csv', 'LOW.US_D1.csv', 'LRCX.US_D1.csv', 'LULU.US_D1.csv', 'LUV.US_D1.csv', 'LVS.US_D1.csv', 'LW.US_D1.csv', 'LYB.US_D1.csv', 'LYV.US_D1.csv', 'MA.US_D1.csv', 'MAA.US_D1.csv', 'MAR.US_D1.csv', 'MAS.US_D1.csv', 'MCD.US_D1.csv', 'MCHP.US_D1.csv', 'MCK.US_D1.csv', 'MCO.US_D1.csv', 'MDB.US_D1.csv', 'MDLZ.US_D1.csv', 'MDT.US_D1.csv', 'MELI.US_D1.csv', 'MET.US_D1.csv', 'META.US_D1.csv', 'MGM.US_D1.csv', 'MHK.US_D1.csv', 'MKC.US_D1.csv', 'MKTX.US_D1.csv', 'MLM.US_D1.csv', 'MMC.US_D1.csv', 'MMM.US_D1.csv', 'MNST.US_D1.csv', 'MO.US_D1.csv', 'MOH.US_D1.csv', 'MOS.US_D1.csv', 'MPC.US_D1.csv', 'MPWR.US_D1.csv', 'MRK.US_D1.csv', 'MRNA.US_D1.csv', 'MRO.US_D1.csv', 'MRVL.US_D1.csv', 'MS.US_D1.csv', 'MSCI.US_D1.csv', 'MSFT.US_D1.csv', 'MSI.US_D1.csv', 'MTB.US_D1.csv', 'MTCH.US_D1.csv', 'MTD.US_D1.csv', 'MU.US_D1.csv', 'NCLH.US_D1.csv', 'NDAQ.US_D1.csv', 'NDSN.US_D1.csv', 'NEE.US_D1.csv', 'NEM.US_D1.csv', 'NFLX.US_D1.csv', 'NI.US_D1.csv', 'NKE.US_D1.csv', 'NOC.US_D1.csv', 'NOW.US_D1.csv', 'NRG.US_D1.csv', 'NSC.US_D1.csv', 'NTAP.US_D1.csv', 'NTRS.US_D1.csv', 'NUE.US_D1.csv', 'NVDA.US_D1.csv', 'NVR.US_D1.csv', 'NWS.US_D1.csv', 'NWSA.US_D1.csv', 'NXPI.US_D1.csv', 'O.US_D1.csv', 'ODFL.US_D1.csv', 'OKE.US_D1.csv', 'OMC.US_D1.csv', 'ON.US_D1.csv', 'ORCL.US_D1.csv', 'ORLY.US_D1.csv', 'OTIS.US_D1.csv', 'OXY.US_D1.csv', 'PANW.US_D1.csv', 'PARA.US_D1.csv', 'PAYC.US_D1.csv', 'PAYX.US_D1.csv', 'PCAR.US_D1.csv', 'PCG.US_D1.csv', 'PDD.US_D1.csv', 'PEAK.US_D1.csv', 'PEG.US_D1.csv', 'PEP.US_D1.csv', 'PFE.US_D1.csv', 'PFG.US_D1.csv', 'PG.US_D1.csv', 'PGR.US_D1.csv', 'PH.US_D1.csv', 'PHM.US_D1.csv', 'PKG.US_D1.csv', 'PLD.US_D1.csv', 'PM.US_D1.csv', 'PNC.US_D1.csv', 'PNR.US_D1.csv', 'PNW.US_D1.csv', 'PODD.US_D1.csv', 'POOL.US_D1.csv', 'PPG.US_D1.csv', 'PPL.US_D1.csv', 'PRU.US_D1.csv', 'PSA.US_D1.csv', 'PSX.US_D1.csv', 'PTC.US_D1.csv', 'PWR.US_D1.csv', 'PXD.US_D1.csv', 'PYPL.US_D1.csv', 'QCOM.US_D1.csv', 'QRVO.US_D1.csv', 'RCL.US_D1.csv', 'REG.US_D1.csv', 'REGN.US_D1.csv', 'RF.US_D1.csv', 'RHI.US_D1.csv', 'RJF.US_D1.csv', 'RL.US_D1.csv', 'RMD.US_D1.csv', 'ROK.US_D1.csv', 'ROL.US_D1.csv', 'ROP.US_D1.csv', 'ROST.US_D1.csv', 'RSG.US_D1.csv', 'RTX.US_D1.csv', 'RVTY.US_D1.csv', 'SBAC.US_D1.csv', 'SBUX.US_D1.csv', 'SCHW.US_D1.csv', 'SHW.US_D1.csv', 'SIRI.US_D1.csv', 'SJM.US_D1.csv', 'SLB.US_D1.csv', 'SNA.US_D1.csv', 'SNPS.US_D1.csv', 'SO.US_D1.csv', 'SPG.US_D1.csv', 'SPGI.US_D1.csv', 'SPLK.US_D1.csv', 'SRE.US_D1.csv', 'STE.US_D1.csv', 'STLD.US_D1.csv', 'STT.US_D1.csv', 'STX.US_D1.csv', 'STZ.US_D1.csv', 'SWK.US_D1.csv', 'SWKS.US_D1.csv', 'SYF.US_D1.csv', 'SYK.US_D1.csv', 'SYY.US_D1.csv', 'T.US_D1.csv', 'TAP.US_D1.csv', 'TDG.US_D1.csv', 'TDY.US_D1.csv', 'TEAM.US_D1.csv', 'TECH.US_D1.csv', 'TEL.US_D1.csv', 'TER.US_D1.csv', 'TFC.US_D1.csv', 'TFX.US_D1.csv', 'TGT.US_D1.csv', 'TJX.US_D1.csv', 'TMO.US_D1.csv', 'TMUS.US_D1.csv', 'TPR.US_D1.csv', 'TRGP.US_D1.csv', 'TRMB.US_D1.csv', 'TROW.US_D1.csv', 'TRV.US_D1.csv', 'TSCO.US_D1.csv', 'TSLA.US_D1.csv', 'TSN.US_D1.csv', 'TT.US_D1.csv', 'TTD.US_D1.csv', 'TTWO.US_D1.csv', 'TXN.US_D1.csv', 'TXT.US_D1.csv', 'TYL.US_D1.csv', 'UAL.US_D1.csv', 'UBER.US_D1.csv', 'UDR.US_D1.csv', 'UHS.US_D1.csv', 'ULTA.US_D1.csv', 'UNH.US_D1.csv', 'UNP.US_D1.csv', 'UPS.US_D1.csv', 'URI.US_D1.csv', 'USB.US_D1.csv', 'V.US_D1.csv', 'VFC.US_D1.csv', 'VICI.US_D1.csv', 'VLO.US_D1.csv', 'VMC.US_D1.csv', 'VRSK.US_D1.csv', 'VRSN.US_D1.csv', 'VRTX.US_D1.csv', 'VTR.US_D1.csv', 'VTRS.US_D1.csv', 'VZ.US_D1.csv', 'WAB.US_D1.csv', 'WAT.US_D1.csv', 'WBA.US_D1.csv', 'WBD.US_D1.csv', 'WDAY.US_D1.csv', 'WDC.US_D1.csv', 'WEC.US_D1.csv', 'WELL.US_D1.csv', 'WFC.US_D1.csv', 'WHR.US_D1.csv', 'WM.US_D1.csv', 'WMB.US_D1.csv', 'WMT.US_D1.csv', 'WRB.US_D1.csv', 'WRK.US_D1.csv', 'WST.US_D1.csv', 'WTW.US_D1.csv', 'WY.US_D1.csv', 'WYNN.US_D1.csv', 'XEL.US_D1.csv', 'XOM.US_D1.csv', 'XRAY.US_D1.csv', 'XYL.US_D1.csv', 'YUM.US_D1.csv', 'ZBH.US_D1.csv', 'ZBRA.US_D1.csv', 'ZION.US_D1.csv', 'ZS.US_D1.csv', 'ZTS.US_D1.csv']\n"
     ]
    }
   ],
   "source": [
    "# dir = os.path.join('dataset', 'kaggle-dse')\n",
    "# dir = r\"/Users/md.raihansobhan/Desktop/BUET/4-2 Sessionals/CSE 472 | ML/online_class/Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models/datasets/NIFTY\"\n",
    "\n",
    "if os.name == 'nt':  # Windows\n",
    "    dir = r\"D:\\Academics\\4-2\\19 Batch\\Sessionals\\CSE 472\\Project\\1905095_1905115\\Github\\Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models\\datasets\\SNP_500\\D1\"\n",
    "else:  # macOS/Linux\n",
    "    dir = r\"/Users/md.raihansobhan/Desktop/BUET/4-2 Sessionals/CSE 472 | ML/online_class/Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models/datasets/SNP_500/D1\"\n",
    "\n",
    "\n",
    "files = os.listdir(dir)\n",
    "files.sort()\n",
    "files = [f for f in files if f not in ['sp500_companies.csv', 'sp500_index.csv']]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Determine input directory based on the operating system\n",
    "if os.name == 'nt':  # Windows\n",
    "    input_dir = r\"D:\\Academics\\4-2\\19 Batch\\Sessionals\\CSE 472\\Project\\1905095_1905115\\Github\\Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models\\datasets\\SNP_500\\D1\"\n",
    "else:  # macOS/Linux\n",
    "    input_dir = r\"/Users/md.raihansobhan/Desktop/BUET/4-2 Sessionals/CSE 472 | ML/online_class/Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models/datasets/SNP_500/D1\"  # Replace with the actual path\n",
    "\n",
    "\n",
    "# List all CSV files in the input directory\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Create a folder named 'D1_Modified' if it doesn't exist\n",
    "output_dir = os.path.join(input_dir, 'D1_Modified')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Process each file\n",
    "for file_name in files:\n",
    "    try:\n",
    "        # Extract the company symbol from the file name\n",
    "        symbol = file_name.split('.')[0]  # Extract everything before the first dot\n",
    "        \n",
    "        # Load the CSV file\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add the 'Symbol' column\n",
    "        df['Symbol'] = symbol\n",
    "\n",
    "        # Save the updated CSV to the output folder\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for file {file_name}: {e}\")\n",
    "\n",
    "print(\"All files processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the data from all CSV files\n",
    "data = []\n",
    "\n",
    "if os.name == 'nt':  # Windows\n",
    "    dir = r\"D:\\Academics\\4-2\\19 Batch\\Sessionals\\CSE 472\\Project\\1905095_1905115\\Github\\Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models\\datasets\\SNP_500\\D1\\D1_Modified\"\n",
    "else:  # macOS/Linux\n",
    "    dir = r\"/Users/md.raihansobhan/Desktop/BUET/4-2 Sessionals/CSE 472 | ML/online_class/Strategic-Stock-Trading-with-Deep-Reinforcement-Learning-Models/datasets/SNP_500/D1/D1_Modified\"  # Replace with the actual path\n",
    "\n",
    "# Loop through the files\n",
    "for f in files:\n",
    "    # Load each CSV file into a DataFrame and append to the list\n",
    "    file_path = os.path.join(dir, f)\n",
    "    data.append(pd.read_csv(file_path))\n",
    "\n",
    "# Check the number of loaded CSV files\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "dfs = []\n",
    "for i in range(1,len(data)):\n",
    "    dfs.append(pd.DataFrame(data[i]))\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'datetime':'date', 'Symbol': 'tic'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique values in the column\n",
    "df = df.drop_duplicates(subset=['date', 'tic'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['date', 'tic'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = df.isna().sum()\n",
    "print(nan_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the zero count of each column\n",
    "zero_counts = df.apply(lambda x: (x==0).sum())\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['low'] != 0]\n",
    "df = df[df['volume'] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the zero count of each column\n",
    "zero_counts = df.apply(lambda x: (x==0).sum())\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the value counts of the 'tic' column\n",
    "tic_counts = df['tic'].value_counts(dropna=False)\n",
    "\n",
    "# Convert the counts to a DataFrame\n",
    "tic_counts_df = tic_counts.reset_index()\n",
    "tic_counts_df.columns = ['Tic', 'Count']\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# tic_counts_df.to_csv('tic_value_counts.csv', index=False)\n",
    "\n",
    "print(tic_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_companies = [\n",
    "    \"JPM\",  # JPMorgan Chase\n",
    "    \"BAC\",  # Bank of America\n",
    "    \"C\",    # Citigroup\n",
    "    \"GS\",   # Goldman Sachs\n",
    "    \"AXP\",  # American Express\n",
    "    \"BLK\",  # BlackRock\n",
    "    \"USB\",  # U.S. Bancorp\n",
    "    \"PNC\",  # PNC Financial Services\n",
    "    \"STT\",  # State Street Corporation\n",
    "    \"SCHW\", # Charles Schwab\n",
    "    \"AIG\",  # American International Group\n",
    "    \"TROW\", # T. Rowe Price\n",
    "    \"FITB\", # Fifth Third Bancorp\n",
    "    \"KEY\",  # KeyCorp\n",
    "    \"COF\",  # Capital One Financial\n",
    "    \"HBAN\", # Huntington Bancshares\n",
    "    \"BK\",   # The Bank of New York Mellon\n",
    "    \"ZION\", # Zions Bancorporation\n",
    "    \"CMA\",  # Comerica\n",
    "    \"MTB\",  # M&T Bank\n",
    "    \"NTRS\", # Northern Trust\n",
    "    \"RF\",   # Regions Financial\n",
    "    \"RJF\",  # Raymond James Financial\n",
    "    \"AMT\",  # American Tower\n",
    "    \"NVDA\", # NVIDIA\n",
    "    \"MSFT\", # Microsoft\n",
    "    \"AAPL\", # Apple\n",
    "    \"AMZN\", # Amazon\n",
    "    \"INTC\", # Intel\n",
    "    \"AMD\",  # Advanced Micro Devices\n",
    "    \"PFE\",  # Pfizer\n",
    "    \"JNJ\",  # Johnson & Johnson\n",
    "    \"MMM\",  # 3M\n",
    "    \"GE\",   # General Electric\n",
    "    \"BA\",   # Boeing\n",
    "    \"CAT\",  # Caterpillar\n",
    "    \"DE\",   # Deere & Company\n",
    "    \"HON\",  # Honeywell\n",
    "    \"WMT\",  # Walmart\n",
    "    \"NEE\",  # NextEra Energy\n",
    "]\n",
    "\n",
    "print(len(top_companies))  # This will output: 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 30 most frequent 'tic' values\n",
    "top_40_tics = top_companies\n",
    "\n",
    "# Filter the DataFrame to keep only these top 30 'tic' values\n",
    "df_filtered = df[df['tic'].isin(top_40_tics)]\n",
    "\n",
    "df = df_filtered.reset_index(drop=True)\n",
    "# Display the filtered DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per year minimum number of trading days\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Create a new column 'year_month' for plotting\n",
    "df['year_month'] = df['year'].astype(str) + '-' + df['month'].astype(str)\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "sns.countplot(x='year_month', data=df, order=sorted(df['year_month'].unique()))\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print df['year'].value_counts() but in year wise sort\n",
    "\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'date' is properly converted to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create an empty DataFrame to store the sampled data\n",
    "sampled_data = pd.DataFrame()\n",
    "\n",
    "# Group the data by year\n",
    "grouped_data = df.groupby('year')\n",
    "\n",
    "# Sample days dynamically based on the number of unique days available in each year\n",
    "for year, group in grouped_data:\n",
    "    # Get the unique dates available for sampling\n",
    "    unique_days = group['date'].nunique()\n",
    "    print('ud '+ str(year)+ ' ' + str(unique_days))\n",
    "    \n",
    "    # Adjust the sample size based on the number of unique days\n",
    "    sample_size = min(208, unique_days)  # Use smaller of 208 or the number of unique days\n",
    "    \n",
    "    # Sample unique days\n",
    "    sampled_days = group['date'].sample(sample_size, replace=False)\n",
    "    \n",
    "\n",
    "    print('ss '+ str(year)+ ' ' + str(sample_size))\n",
    "\n",
    "    # Filter the data for the sampled days\n",
    "    sampled_year_data = group[group['date'].isin(sampled_days)]\n",
    "    \n",
    "    # Ensure each sampled day includes all 30 trading codes\n",
    "    sampled_year_data = sampled_year_data.groupby('date').filter(lambda x: len(x) == 30)\n",
    "    \n",
    "    # Append the sampled data to the final DataFrame\n",
    "    sampled_data = pd.concat([sampled_data, sampled_year_data])\n",
    "\n",
    "# Reset index of the final sampled data\n",
    "sampled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(sampled_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count zeros in the 'Volume' column\n",
    "zero_count_volume = (df['volume'] == 0).sum()\n",
    "\n",
    "print(\"Number of zeros in 'Volume' column:\", zero_count_volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the zero count of each column\n",
    "zero_counts = df.apply(lambda x: (x==0).sum())\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL\")\n",
    "\n",
    "import itertools\n",
    "\n",
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['year_month','month','year'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'close' is the column with closing prices\n",
    "std_dev = df.groupby('tic')['close'].std()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(std_dev, bins=50, kde=True)\n",
    "plt.title('Distribution of std of the stocks')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "if os.name == 'nt':  # Windows\n",
    "    # plt.savefig('Output\\std_dev.png')\n",
    "    do_nothing = 1\n",
    "else:\n",
    "    plt.savefig('Output/output_snp500.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = pd.to_datetime(df['date']).dt.dayofweek\n",
    "df['short_resistance']= df['high'].rolling(window=10,min_periods=0).max()\n",
    "df['short_support']= df['low'].rolling(window=10,min_periods=0).min()\n",
    "df['long_resistance']= df['high'].rolling(window=50,min_periods=0).max()\n",
    "df['long_support']= df['low'].rolling(window=50,min_periods=0).min()\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values(by=['date','tic']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime to string\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "df_price_pivot = data.pivot(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "df_price_pivot = df_price_pivot.pct_change()\n",
    "unique_date = data.date.unique()\n",
    "# start after a year\n",
    "start = 208\n",
    "turbulence_index = [0] * start\n",
    "# turbulence_index = [0]\n",
    "count = 0\n",
    "for i in range(start, len(unique_date)):\n",
    "  current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
    "  # use one year rolling window to calcualte covariance\n",
    "  hist_price = df_price_pivot[\n",
    "      (df_price_pivot.index < unique_date[i])\n",
    "      & (df_price_pivot.index >= unique_date[i - 208])\n",
    "  ]\n",
    "  # Drop tickers which has number missing values more than the \"oldest\" ticker\n",
    "  filtered_hist_price = hist_price.iloc[\n",
    "      hist_price.isna().sum().min() :\n",
    "  ].dropna(axis=1)\n",
    "\n",
    "  cov_temp = filtered_hist_price.cov()\n",
    "  current_temp = current_price[[x for x in filtered_hist_price]] - np.mean(\n",
    "      filtered_hist_price, axis=0\n",
    "  )\n",
    "  # cov_temp = hist_price.cov()\n",
    "  # current_temp=(current_price - np.mean(hist_price,axis=0))\n",
    "\n",
    "  temp = current_temp.values.dot(np.linalg.pinv(cov_temp)).dot(\n",
    "      current_temp.values.T\n",
    "  )\n",
    "  if temp > 0:\n",
    "      count += 1\n",
    "      if count > 2:\n",
    "          turbulence_temp = temp[0][0]\n",
    "      else:\n",
    "          # avoid large outlier because of the calculation just begins\n",
    "          turbulence_temp = 0\n",
    "  else:\n",
    "      turbulence_temp = 0\n",
    "  turbulence_index.append(turbulence_temp)\n",
    "print(len(turbulence_index))\n",
    "try:\n",
    "  turbulence_index = pd.DataFrame(\n",
    "      {\"date\": df_price_pivot.index, \"turbulence\": turbulence_index}\n",
    "  )\n",
    "except ValueError:\n",
    "  raise Exception(\"Turbulence information could not be added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbulence_index['turbulence'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(turbulence_index, on=\"date\")\n",
    "df['tic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "nan_counts = df.isna().sum()\n",
    "print(nan_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] <= '2023-12-31']\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=False,\n",
    "                    use_turbulence=False,\n",
    "                    user_defined_feature = False)\n",
    "# turbulance is giving error\n",
    "df = df.fillna(value = 0)\n",
    "processed = fe.preprocess_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "nan_counts = df.isna().sum()\n",
    "print(nan_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed['tic'].nunique())\n",
    "print(processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_count = processed_full['tic'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_full.shape)\n",
    "processed_full.sort_values(['date','tic'],ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvo_df = processed_full.sort_values(['date','tic'],ignore_index=True)[['date','tic','close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minimum date value from the 'date' column\n",
    "min_date = processed_full['date'].min()\n",
    "print(f\"Minimum Date: {min_date}\")\n",
    "\n",
    "# Get the maximum date value from the 'date' column\n",
    "max_date = processed_full['date'].max()\n",
    "print(f\"Maximum Date: {max_date}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '1998-01-02'\n",
    "TRAIN_END_DATE = '2018-12-31'\n",
    "TRADE_START_DATE = '2019-01-01'\n",
    "TRADE_END_DATE = '2023-12-30' ## CARE IN NIFTY\n",
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "# print(stock_dimension)\n",
    "\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.0005] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 2000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "    # \"cash_penalty_percentage\": 0.1,\n",
    "    # \"exponential_cash_average\": 0.04\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/SNP_500/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=run_timesteps) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/SNP_500/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg,\n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=run_timesteps) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/SNP_500/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=run_timesteps) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 100000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/SNP_500/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=run_timesteps) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/SNP_500/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=run_timesteps) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_risk_indicator = processed_full[(processed_full.date<TRAIN_END_DATE) & (processed_full.date>=TRAIN_START_DATE)]\n",
    "insample_risk_indicator = data_risk_indicator.drop_duplicates(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_risk_indicator.turbulence.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "filtered_turbulence = insample_risk_indicator.turbulence[insample_risk_indicator.turbulence < 600]\n",
    "sns.histplot(filtered_turbulence, bins=50, kde=True)\n",
    "plt.title('Distribution of turbulence of DJI top 30 stocks')\n",
    "plt.xlabel('Turbulence')\n",
    "plt.ylabel('Frequency')\n",
    "if os.name == 'nt':  # Windows\n",
    "    # plt.savefig('Output\\turbulence_dse.png')\n",
    "    do_nothing = 1\n",
    "else:\n",
    "    plt.savefig('Output/turbulence_snp500.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_risk_indicator.turbulence.quantile(0.996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade,  **env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_moedl = trained_a2c\n",
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_moedl,\n",
    "    environment = e_trade_gym)\n",
    "\n",
    "# Save the actions DataFrame to a CSV file\n",
    "# df_actions_a2c.to_csv(\"actions_a2c.csv\", index=True)  # Include index for date information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_moedl = trained_ddpg\n",
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_moedl,\n",
    "    environment = e_trade_gym)\n",
    "# df_actions_ddpg.to_csv(\"actions_ddpg.csv\", index=True)  # Include index for date information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_moedl = trained_ppo\n",
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_moedl,\n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_moedl = trained_td3\n",
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_moedl,\n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_moedl = trained_sac\n",
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_moedl,\n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max of each model\n",
    "print('A2C:', df_account_value_a2c['account_value'].max())\n",
    "print('DDPG:', df_account_value_ddpg['account_value'].max())\n",
    "print('PPO:', df_account_value_ppo['account_value'].max())\n",
    "print('TD3:', df_account_value_td3['account_value'].max())\n",
    "print('SAC:', df_account_value_sac['account_value'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save account value and action\n",
    "\n",
    "# A2C\n",
    "root_dir = 'SNP_500'\n",
    "sub_dir = 'account_values_actions_SNP500'\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "if not os.path.exists(os.path.join(root_dir, sub_dir)):\n",
    "    os.mkdir(os.path.join(root_dir, sub_dir))\n",
    "\n",
    "root_dir = os.path.join(root_dir, sub_dir)\n",
    "file_path = os.path.join(root_dir, 'df_account_value_a2c.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_account_value_a2c, f)\n",
    "file_path = os.path.join(root_dir, 'df_actions_a2c.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_actions_a2c, f)\n",
    "\n",
    "# DDPG\n",
    "file_path = os.path.join(root_dir, 'df_account_value_ddpg.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_account_value_ddpg, f)\n",
    "\n",
    "file_path = os.path.join(root_dir, 'df_actions_ddpg.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_actions_ddpg, f)\n",
    "\n",
    "# PPO\n",
    "file_path = os.path.join(root_dir, 'df_account_value_ppo.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_account_value_ppo, f)\n",
    "\n",
    "file_path = os.path.join(root_dir, 'df_actions_ppo.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_actions_ppo, f)\n",
    "\n",
    "# TD3\n",
    "file_path = os.path.join(root_dir, 'df_account_value_td3.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_account_value_td3, f)\n",
    "\n",
    "file_path = os.path.join(root_dir, 'df_actions_td3.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_actions_td3, f)\n",
    "\n",
    "# SAC\n",
    "file_path = os.path.join(root_dir, 'df_account_value_sac.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_account_value_sac, f)\n",
    "\n",
    "file_path = os.path.join(root_dir, 'df_actions_sac.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(df_actions_sac, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst = mvo_df\n",
    "fst = fst.iloc[0*stock_dimension:0*stock_dimension+stock_dimension, :]\n",
    "tic = fst['tic'].tolist()\n",
    "\n",
    "mvo = pd.DataFrame()\n",
    "\n",
    "for k in range(len(tic)):\n",
    "  mvo[tic[k]] = 0\n",
    "\n",
    "for i in range(mvo_df.shape[0]//stock_dimension):\n",
    "  n = mvo_df\n",
    "  n = n.iloc[i*stock_dimension:i*stock_dimension+stock_dimension, :]\n",
    "  date = n['date'][i*stock_dimension]\n",
    "  mvo.loc[date] = n['close'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvo.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize \n",
    "from scipy.optimize import linprog\n",
    "\n",
    "#function obtains maximal return portfolio using linear programming\n",
    "\n",
    "def MaximizeReturns(MeanReturns, PortfolioSize):\n",
    "    \n",
    "  #dependencies\n",
    "  \n",
    "    \n",
    "  c = (np.multiply(-1, MeanReturns))\n",
    "  A = np.ones([PortfolioSize,1]).T\n",
    "  b=[1]\n",
    "  res = linprog(c, A_ub = A, b_ub = b, bounds = (0,1), method = 'simplex') \n",
    "    \n",
    "  return res\n",
    "\n",
    "def MinimizeRisk(CovarReturns, PortfolioSize):\n",
    "    \n",
    "  def f(x, CovarReturns):\n",
    "    func = np.matmul(np.matmul(x, CovarReturns), x.T) \n",
    "    return func\n",
    "\n",
    "  def constraintEq(x):\n",
    "    A=np.ones(x.shape)\n",
    "    b=1\n",
    "    constraintVal = np.matmul(A,x.T)-b \n",
    "    return constraintVal\n",
    "    \n",
    "  xinit=np.repeat(0.1, PortfolioSize)\n",
    "  cons = ({'type': 'eq', 'fun':constraintEq})\n",
    "  lb = 0\n",
    "  ub = 1\n",
    "  bnds = tuple([(lb,ub) for x in xinit])\n",
    "\n",
    "  opt = optimize.minimize (f, x0 = xinit, args = (CovarReturns),  bounds = bnds, \\\n",
    "                             constraints = cons, tol = 10**-3)\n",
    "    \n",
    "  return opt\n",
    "\n",
    "def MinimizeRiskConstr(MeanReturns, CovarReturns, PortfolioSize, R):\n",
    "    \n",
    "  def  f(x,CovarReturns):\n",
    "         \n",
    "    func = np.matmul(np.matmul(x,CovarReturns ), x.T)\n",
    "    return func\n",
    "\n",
    "  def constraintEq(x):\n",
    "    AEq=np.ones(x.shape)\n",
    "    bEq=1\n",
    "    EqconstraintVal = np.matmul(AEq,x.T)-bEq \n",
    "    return EqconstraintVal\n",
    "    \n",
    "  def constraintIneq(x, MeanReturns, R):\n",
    "    AIneq = np.array(MeanReturns)\n",
    "    bIneq = R\n",
    "    IneqconstraintVal = np.matmul(AIneq,x.T) - bIneq\n",
    "    return IneqconstraintVal\n",
    "    \n",
    "\n",
    "  xinit=np.repeat(0.1, PortfolioSize)\n",
    "  cons = ({'type': 'eq', 'fun':constraintEq},\n",
    "          {'type':'ineq', 'fun':constraintIneq, 'args':(MeanReturns,R) })\n",
    "  lb = 0\n",
    "  ub = 1\n",
    "  bnds = tuple([(lb,ub) for x in xinit])\n",
    "\n",
    "  opt = optimize.minimize (f, args = (CovarReturns), method ='trust-constr',  \\\n",
    "                x0 = xinit,   bounds = bnds, constraints = cons, tol = 10**-3)\n",
    "    \n",
    "  return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StockReturnsComputing(StockPrice, Rows, Columns): \n",
    "  import numpy as np \n",
    "  StockReturn = np.zeros([Rows-1, Columns]) \n",
    "  for j in range(Columns):        # j: Assets \n",
    "    for i in range(Rows-1):     # i: Daily Prices \n",
    "      StockReturn[i,j]=((StockPrice[i+1, j]-StockPrice[i,j])/StockPrice[i,j])* 100 \n",
    "      \n",
    "  return StockReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain optimal portfolio sets that maximize return and minimize risk\n",
    "\n",
    "#Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#input k-portfolio 1 dataset comprising 15 stocks\n",
    "# StockFileName = './DJIA_Apr112014_Apr112019_kpf1.csv'\n",
    "\n",
    "Rows = mvo.shape[0] #number of rows\n",
    "Columns = stock_dimension  #excluding date\n",
    "portfolioSize = stock_dimension #set portfolio size\n",
    "\n",
    "#read stock prices in a dataframe\n",
    "# df = pd.read_csv(StockFileName,  nrows= Rows)\n",
    "\n",
    "#extract asset labels\n",
    "# assetLabels = df.columns[1:Columns+1].tolist()\n",
    "# print(assetLabels)\n",
    "\n",
    "#extract asset prices\n",
    "# StockData = df.iloc[0:, 1:]\n",
    "StockData = mvo[mvo.index <= TRAIN_END_DATE]\n",
    "TradeData = mvo[mvo.index > TRADE_START_DATE]\n",
    "# df.head()\n",
    "TradeData.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute asset returns\n",
    "arStockPrices = np.asarray(StockData)\n",
    "[Rows, Cols]=arStockPrices.shape\n",
    "arReturns = StockReturnsComputing(arStockPrices, Rows, Cols)\n",
    "\n",
    "\n",
    "#compute mean returns and variance covariance matrix of returns\n",
    "meanReturns = np.mean(arReturns, axis = 0)\n",
    "covReturns = np.cov(arReturns, rowvar=False)\n",
    " \n",
    "#set precision for printing results\n",
    "np.set_printoptions(precision=3, suppress = True)\n",
    "\n",
    "#display mean returns and variance-covariance matrix of returns\n",
    "print('Mean returns of assets in k-portfolio 1\\n', meanReturns)\n",
    "print('Variance-Covariance matrix of returns\\n', covReturns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "ef_mean = EfficientFrontier(meanReturns, covReturns, weight_bounds=(0, 0.2))\n",
    "raw_weights_mean = ef_mean.max_sharpe()\n",
    "cleaned_weights_mean = ef_mean.clean_weights()\n",
    "mvo_weights = np.array([2000000 * cleaned_weights_mean[i] for i in range(stock_dimension)])\n",
    "mvo_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockData.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastPrice = np.array([1/p for p in StockData.tail(1).to_numpy()[0]])\n",
    "Initial_Portfolio = np.multiply(mvo_weights, LastPrice)\n",
    "Initial_Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio_Assets = TradeData @ Initial_Portfolio\n",
    "MVO_result = pd.DataFrame(Portfolio_Assets, columns=[\"Mean Var\"])\n",
    "MVO_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0])\n",
    "df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0])\n",
    "df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0])\n",
    "df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0])\n",
    "df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0])\n",
    "# df_account_value_a2c.to_csv(\"df_account_value_a2c.csv\")\n",
    "if os.name == 'nt':  # Windows\n",
    "    # df_account_value_a2c.to_csv(\"NIFTY\\df_account_value_a2c.csv\")\n",
    "    do_nothing = 0\n",
    "else:  # macOS/Linux\n",
    "    df_account_value_a2c.to_csv(\"SNP_500/df_account_value_a2c.csv\")\n",
    "result = pd.merge(df_result_a2c, df_result_ddpg, left_index=True, right_index=True, suffixes=('_a2c', '_ddpg'))\n",
    "result = pd.merge(result, df_result_td3, left_index=True, right_index=True, suffixes=('', '_td3'))\n",
    "result = pd.merge(result, df_result_ppo, left_index=True, right_index=True, suffixes=('', '_ppo'))\n",
    "result = pd.merge(result, df_result_sac, left_index=True, right_index=True, suffixes=('', '_sac'))\n",
    "result = pd.merge(result, MVO_result, left_index=True, right_index=True, suffixes=('', '_mvo'))\n",
    "result.columns = ['a2c', 'ddpg', 'td3', 'ppo', 'sac', 'mvo']\n",
    "\n",
    "print(\"result: \", result)\n",
    "print(\"result: \", result)\n",
    "if os.name == 'nt':  # Windows\n",
    "    do_nothing = 1\n",
    "else:\n",
    "    result.to_csv(\"SNP_500/result_nifty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of initial and final value, annual return, sharpe ratio, max drawdown\n",
    "# initial value\n",
    "initial_value = result.iloc[0]\n",
    "# final value\n",
    "final_value = result.iloc[-1]\n",
    "# annual return\n",
    "annual_return = (final_value/initial_value)**(1/3) - 1\n",
    "annual_return = annual_return * 100\n",
    "# sharpe ratio\n",
    "sharpe_ratio = annual_return / result.std()\n",
    "# max drawdown\n",
    "max_drawdown = (result - result.expanding().max()).min()\n",
    "# combine all the metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns from the portfolio values\n",
    "daily_returns = result.pct_change().dropna()\n",
    "\n",
    "# Set target return (e.g., 0 for simplicity, or replace with a risk-free rate if needed)\n",
    "target_return = 0\n",
    "\n",
    "# Calculate downside deviation\n",
    "downside_returns = daily_returns[daily_returns < target_return]  # Only returns below target\n",
    "downside_deviation = np.sqrt((downside_returns**2).mean())  # Root mean square of negative returns\n",
    "\n",
    "# Annual return (already calculated)\n",
    "annual_return = (result.iloc[-1] / result.iloc[0])**(1/3) - 1\n",
    "annual_return = annual_return * 100  # Convert to percentage\n",
    "\n",
    "# Sortino Ratio\n",
    "sortino_ratio = (annual_return - target_return) / downside_deviation\n",
    "\n",
    "# # Create a DataFrame for Sortino Ratio\n",
    "# sortino_ratio_df = pd.DataFrame(sortino_ratio, columns=['Sortino Ratio'])\n",
    "# print(sortino_ratio_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame([initial_value, final_value, annual_return, sharpe_ratio,sortino_ratio, max_drawdown], index=['initial_value', 'final_value', 'annual_return', 'sharpe_ratio','sortino_ratio', 'max_drawdown']).T\n",
    "\n",
    "if os.name == 'nt':  # Windows\n",
    "    # df_account_value_a2c.to_csv(\"NIFTY\\df_account_value_a2c.csv\")\n",
    "    do_nothing = 0\n",
    "else: # macOS/Linux\n",
    "    metrics.to_csv(\"SNP_500/metrics_snp500.csv\")\n",
    "    \n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk (Maximum Drawdown)\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming metrics DataFrame is already created and saved\n",
    "# metrics = pd.read_csv(\"metrics_cashless_snp500.csv\", index_col=0)\n",
    "\n",
    "# Calculate the maximum drawdowns from metrics\n",
    "max_drawdowns = metrics['max_drawdown']\n",
    "\n",
    "# Identify the index of the minimum absolute drawdown\n",
    "min_drawdown_index = max_drawdowns.abs().idxmin()\n",
    "\n",
    "# Define bar colors: light green for the lowest drawdown, sky blue for others\n",
    "colors = ['lightgreen' if index == min_drawdown_index else 'skyblue' for index in max_drawdowns.index]\n",
    "\n",
    "# Plot the maximum drawdowns\n",
    "plt.figure(figsize=(10, 5))\n",
    "max_drawdowns.plot(kind='bar', color=colors, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Maximum Drawdown Comparison', fontsize=14)\n",
    "plt.ylabel('Maximum Drawdown ($)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add grid for clarity\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Annotate bars with values, slightly shifting the labels down\n",
    "for index, value in enumerate(max_drawdowns):\n",
    "    # Adjust the position of the text by subtracting a small offset\n",
    "    offset = -0.025 * max_drawdowns.abs().max()  # 5% of the max absolute value as offset\n",
    "    plt.text(index, value + offset, f'{value:.2e}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "# plt.savefig('max_drawdown_comparison.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profitability\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming metrics DataFrame is already created and saved\n",
    "# metrics = pd.read_csv(\"metrics_cashless_snp500.csv\", index_col=0)\n",
    "\n",
    "# Extract the final portfolio values and annual returns from metrics\n",
    "final_values = metrics['final_value']\n",
    "annual_returns = metrics['annual_return']\n",
    "\n",
    "# Identify the index of the maximum and second maximum final value and annual return\n",
    "max_final_value_index = final_values.idxmax()\n",
    "second_max_final_value_index = final_values.nlargest(2).idxmin()\n",
    "max_annual_return_index = annual_returns.idxmax()\n",
    "second_max_annual_return_index = annual_returns.nlargest(2).idxmin()\n",
    "\n",
    "# Define bar colors: light coral for the highest value, light green for the second highest, sky blue for others\n",
    "colors_final_value = [\n",
    "    'lightcoral' if index == max_final_value_index else 'lightgreen' if index == second_max_final_value_index else 'skyblue'\n",
    "    for index in final_values.index\n",
    "]\n",
    "colors_annual_return = [\n",
    "    'lightcoral' if index == max_annual_return_index else 'lightgreen' if index == second_max_annual_return_index else 'skyblue'\n",
    "    for index in annual_returns.index\n",
    "]\n",
    "\n",
    "# Plot the final portfolio values\n",
    "plt.figure(figsize=(10, 5))\n",
    "final_values.plot(kind='bar', color=colors_final_value, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Final Portfolio Value Comparison', fontsize=14)\n",
    "plt.ylabel('Final Portfolio Value ($)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add grid for clarity\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Annotate bars with values, slightly shifting the labels down\n",
    "for index, value in enumerate(final_values):\n",
    "    offset = -0.025 * final_values.max()  # 2.5% of the max value as offset\n",
    "    plt.text(index, value + offset, f'{value:.2e}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Draw black dotted lines for the highest and second-highest final values\n",
    "plt.axhline(y=final_values[max_final_value_index], color='black', linestyle='--', linewidth=1)\n",
    "plt.axhline(y=final_values[second_max_final_value_index], color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "# plt.savefig('final_value_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot the annual returns\n",
    "plt.figure(figsize=(10, 5))\n",
    "annual_returns.plot(kind='bar', color=colors_annual_return, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Annual Return Comparison', fontsize=14)\n",
    "plt.ylabel('Annual Return (%)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add grid for clarity\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Annotate bars with values, slightly shifting the labels down\n",
    "for index, value in enumerate(annual_returns):\n",
    "    offset = -0.025 * annual_returns.max()  # 2.5% of the max value as offset\n",
    "    plt.text(index, value + offset, f'{value:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Draw black dotted lines for the highest and second-highest annual returns\n",
    "plt.axhline(y=annual_returns[max_annual_return_index], color='black', linestyle='--', linewidth=1)\n",
    "plt.axhline(y=annual_returns[second_max_annual_return_index], color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "# plt.savefig('annual_return_comparison.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency (Volatility of Returns)\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract the year from the 'date' column\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Count the number of trading days per year\n",
    "trading_days_per_year = df.groupby('year')['date'].nunique()\n",
    "\n",
    "# Display the result\n",
    "print(trading_days_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "daily_returns = result.pct_change().dropna()\n",
    "\n",
    "# Calculate annualized volatility\n",
    "volatility = daily_returns.std() * np.sqrt(252)  # 252 trading days assumed for a year\n",
    "\n",
    "# Add volatility to metrics DataFrame\n",
    "metrics = pd.DataFrame(\n",
    "    [initial_value, final_value, annual_return, sharpe_ratio, sortino_ratio, max_drawdown, volatility],\n",
    "    index=['initial_value', 'final_value', 'annual_return', 'sharpe_ratio', 'sortino_ratio', 'max_drawdown', 'volatility']\n",
    ").T\n",
    "\n",
    "# Save and display metrics\n",
    "# metrics.to_csv(\"metrics_cashless_snp500.csv\")\n",
    "print(metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bar colors: light green for the lowest value, sky blue for others\n",
    "colors_volatility = ['lightgreen' if value == metrics['volatility'].min() else 'skyblue' for value in metrics['volatility']]\n",
    "\n",
    "# Plot the volatility comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "metrics['volatility'].plot(kind='bar', color=colors_volatility, edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Annualized Volatility Comparison', fontsize=14)\n",
    "plt.ylabel('Annualized Volatility', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Add grid for clarity\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Annotate bars with values, slightly shifting the labels down\n",
    "for index, value in enumerate(metrics['volatility']):\n",
    "    offset = -0.025 * metrics['volatility'].max()  # 2.5% of the max value as offset\n",
    "    plt.text(index, value + offset, f'{value:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "# plt.savefig('volatility_comparison.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggresiveness\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate profitability\n",
    "profitability = metrics['final_value'] - metrics['initial_value']\n",
    "\n",
    "# Calculate aggressiveness\n",
    "aggressiveness = profitability / metrics['max_drawdown']\n",
    "\n",
    "# Add aggressiveness to metrics DataFrame\n",
    "metrics = pd.DataFrame(\n",
    "    [initial_value, final_value, annual_return, sharpe_ratio, sortino_ratio, max_drawdown, volatility, profitability, aggressiveness],\n",
    "    index=['initial_value', 'final_value', 'annual_return', 'sharpe_ratio', 'sortino_ratio', 'max_drawdown', 'volatility', 'profitability', 'aggressiveness']\n",
    ").T\n",
    "\n",
    "# Save and display metrics\n",
    "# metrics.to_csv(\"metrics_cashless_snp500.csv\")\n",
    "print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the aggressiveness comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "metrics['aggressiveness'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Aggressiveness Comparison', fontsize=14)\n",
    "plt.ylabel('Aggressiveness', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Annotate bars with values, slightly shifting the labels down\n",
    "for index, value in enumerate(metrics['aggressiveness']):\n",
    "    offset = -0.085 * metrics['aggressiveness'].max()  # 2.5% of the max value as offset\n",
    "    plt.text(index, value + offset, f'{value:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Show grid for better visualization\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "# plt.savefig('aggressiveness_comparison.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawdown Recovery Time\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate drawdown recovery time for a2c using df_account_value_a2c\n",
    "\n",
    "# 1. identift maximum and minimum portfolio value\n",
    "max_portfolio_value = df_account_value_a2c['account_value'].max()\n",
    "min_portfolio_value = df_account_value_a2c['account_value'].min()\n",
    "\n",
    "# 2. calculate drawdown\n",
    "drawdown = (max_portfolio_value - min_portfolio_value)\n",
    "\n",
    "# 3. Find recovery date\n",
    "recovery_date = df_account_value_a2c[df_account_value_a2c['account_value'] == max_portfolio_value].index[0]\n",
    "\n",
    "# 4. Find lowest point date\n",
    "lowest_point_date = df_account_value_a2c[df_account_value_a2c['account_value'] == min_portfolio_value].index[0]\n",
    "\n",
    "# 5. Find drawdown recovery time\n",
    "drawdown_recovery_time = recovery_date - lowest_point_date\n",
    "\n",
    "# 6. Display the dates\n",
    "print(f\"Lowest Point Date: {lowest_point_date}\")\n",
    "print(f\"Recovery Date: {recovery_date}\")\n",
    "\n",
    "# 7. Display the drawdown and recovery time\n",
    "print(f\"Drawdown: {drawdown:.2f}\")\n",
    "print(f\"Drawdown Recovery Time: {drawdown_recovery_time} days\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime  import datetime\n",
    "\n",
    "# 1. Identifying maximum and minimum portfolio value\n",
    "max_portfolio_value = df_account_value_a2c['account_value'].max()\n",
    "min_portfolio_value = df_account_value_a2c['account_value'].min()\n",
    "\n",
    "# 2. Calculate drawdown\n",
    "drawdown = (max_portfolio_value - min_portfolio_value)\n",
    "\n",
    "# 3. Find recovery date\n",
    "highest_point_date = df_account_value_a2c[df_account_value_a2c['account_value'] == max_portfolio_value]['date'].iloc[0]\n",
    "\n",
    "# 4. Find lowest point date\n",
    "lowest_point_date = df_account_value_a2c[df_account_value_a2c['account_value'] == min_portfolio_value]['date'].iloc[0]\n",
    "\n",
    "# 5. Find drawdown recovery time (in days)\n",
    "\n",
    "# Convert strings to datetime objects\n",
    "highest_point_date_dt = datetime.strptime(highest_point_date, '%Y-%m-%d')\n",
    "lowest_point_date_dt = datetime.strptime(lowest_point_date, '%Y-%m-%d')\n",
    "# Calculate the difference in days\n",
    "days_difference = (highest_point_date_dt - lowest_point_date_dt).days\n",
    "\n",
    "print(f\"Lowest Point Date: {lowest_point_date}\")\n",
    "print(f\"Highest Point Date: {highest_point_date}\")\n",
    "print(f\"Drawdown Recovery Time: {days_difference} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def calculate_recovery_time(arg_df, algorithm_name):\n",
    "    \n",
    "\n",
    "    # 1. Identifying maximum and minimum portfolio value\n",
    "    max_portfolio_value = arg_df['account_value'].max()\n",
    "    min_portfolio_value = arg_df['account_value'].min()\n",
    "\n",
    "    # 2. Calculate drawdown\n",
    "    drawdown = max_portfolio_value - min_portfolio_value\n",
    "\n",
    "    # 3. Find recovery date\n",
    "    highest_point_date = arg_df[arg_df['account_value'] == max_portfolio_value]['date'].iloc[0]\n",
    "\n",
    "\n",
    "    # 4. Find lowest point date\n",
    "    lowest_point_date = arg_df[arg_df['account_value'] == min_portfolio_value]['date'].iloc[0]\n",
    "\n",
    "    # 5. Find drawdown recovery time (in days)\n",
    "    highest_point_date_dt = datetime.strptime(highest_point_date, '%Y-%m-%d')\n",
    "    lowest_point_date_dt = datetime.strptime(lowest_point_date, '%Y-%m-%d')\n",
    "\n",
    "    # Calculate the difference in days\n",
    "    drt = (highest_point_date_dt - lowest_point_date_dt).days\n",
    "\n",
    "    return {\n",
    "        'algorithm': algorithm_name,\n",
    "        'drawdown': drawdown,\n",
    "        'max_portfolio_value': max_portfolio_value,\n",
    "        'highest_point_date': highest_point_date,\n",
    "        'min_portfolio_value': min_portfolio_value,\n",
    "        'lowest_point_date': lowest_point_date,\n",
    "        'recovery_time': drt\n",
    "    }\n",
    "\n",
    "# Example usage for PPO\n",
    "ppo_recovery = calculate_recovery_time(df_account_value_ppo, 'ppo')\n",
    "print(ppo_recovery)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "copy_result = result.copy()\n",
    "df_account_value_mvo_temp = copy_result.drop(columns=['a2c', 'ddpg', 'td3', 'ppo', 'sac'])\n",
    "df_account_value_mvo_temp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_mvo = df_account_value_mvo_temp.reset_index()  # Reset the index to convert dates to a column\n",
    "df_account_value_mvo.columns = ['date', 'account_value']  # Rename the columns\n",
    "df_account_value_mvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_recovery = calculate_recovery_time(df_account_value_a2c, 'a2c')\n",
    "ddpg_recovery = calculate_recovery_time(df_account_value_ddpg, 'ddpg')\n",
    "td3_recovery = calculate_recovery_time(df_account_value_td3, 'td3')\n",
    "ppo_recovery = calculate_recovery_time(df_account_value_ppo, 'ppo')\n",
    "sac_recovery = calculate_recovery_time(df_account_value_sac, 'sac')\n",
    "mvo_recovery = calculate_recovery_time(df_account_value_mvo, 'mvo')\n",
    "\n",
    "# Display the recovery time for each model\n",
    "print(a2c_recovery)\n",
    "print(ddpg_recovery)\n",
    "print(td3_recovery)\n",
    "print(ppo_recovery)\n",
    "print(sac_recovery)\n",
    "print(mvo_recovery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a array of all the recovery time\n",
    "recovery_times = [a2c_recovery['recovery_time'], ddpg_recovery['recovery_time'], td3_recovery['recovery_time'], ppo_recovery['recovery_time'], sac_recovery['recovery_time'], mvo_recovery['recovery_time']]\n",
    "recovery_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['recovery_time'] = recovery_times\n",
    "# metrics.to_csv(\"metrics_cashless_snp500.csv\")\n",
    "\n",
    "metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the recovery time comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "metrics['recovery_time'].plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Recovery Time Comparison', fontsize=14)\n",
    "plt.ylabel('Recovery Time (days)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Annotate bars with values, slightly shifting the labels down\n",
    "for index, value in enumerate(metrics['recovery_time']):\n",
    "    offset = 0.035 * metrics['recovery_time'].max()  # 3.5% of the max value as offset\n",
    "    plt.text(index, value + offset, f'{value}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Add a red dotted line across y=0\n",
    "plt.axhline(y=0, color='black', linestyle='dotted', linewidth=1.5)\n",
    "\n",
    "# Show grid for better visualization\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "# plt.savefig('recovery_time_comparison.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeatMap\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming metrics DataFrame is already created\n",
    "# metrics = pd.read_csv(\"metrics_cashless_snp500.csv\")\n",
    "\n",
    "# Selecting relevant columns for heatmap\n",
    "metrics_subset = metrics[['annual_return', 'sharpe_ratio', 'sortino_ratio', \n",
    "                         'max_drawdown', 'volatility', 'profitability', \n",
    "                         'aggressiveness', 'recovery_time']]\n",
    "\n",
    "# Normalize data if needed\n",
    "metrics_normalized = (metrics_subset - metrics_subset.min()) / (metrics_subset.max() - metrics_subset.min())\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap = sns.heatmap(metrics_normalized, annot=True, cmap='RdYlGn', center=0,\n",
    "                      cbar_kws={'label': 'Normalized Value'},\n",
    "                      linewidths=0.5)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Comparison of Different Algorithms Across Metrics', fontsize=16)\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('Algorithms', fontsize=12)\n",
    "\n",
    "# Display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same heatmap but without the mvo model\n",
    "\n",
    "# Selecting relevant columns for heatmap\n",
    "metrics_subset = metrics[['annual_return', 'sharpe_ratio', 'sortino_ratio', \n",
    "                         'max_drawdown', 'volatility', 'profitability', \n",
    "                         'aggressiveness', 'recovery_time']].drop('mvo')\n",
    "\n",
    "# Normalize data if needed\n",
    "metrics_normalized = (metrics_subset - metrics_subset.min()) / (metrics_subset.max() - metrics_subset.min())\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap = sns.heatmap(metrics_normalized, annot=True, cmap='RdYlGn', center=0,\n",
    "                      cbar_kws={'label': 'Normalized Value'},\n",
    "                      linewidths=0.5)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Comparison of Different Algorithms Across Metrics', fontsize=16)\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('Algorithms', fontsize=12)\n",
    "\n",
    "# Display plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Plot\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics) \n",
    "# Assuming metrics DataFrame\n",
    "algorithms = ['a2c', 'ddpg', 'td3', 'ppo', 'sac', 'mvo']\n",
    "\n",
    "metrics_subset = metrics.loc[algorithms, ['final_value','annual_return', 'sharpe_ratio', 'sortino_ratio', \n",
    "                                          'max_drawdown', 'volatility', 'profitability', \n",
    "                                          'aggressiveness', 'recovery_time']]\n",
    "\n",
    "metrics_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "# Assuming metrics DataFrame\n",
    "algorithms = ['a2c', 'ddpg', 'td3', 'ppo', 'sac', 'mvo']\n",
    "\n",
    "# Extract metrics for all algorithms\n",
    "metrics_subset = metrics.loc[algorithms, ['annual_return', 'sharpe_ratio'   , 'final_value', \n",
    "                                          'sortino_ratio', 'max_drawdown'   , 'volatility', \n",
    "                                          'profitability', 'aggressiveness' , 'recovery_time']]\n",
    "\n",
    "# Normalize metrics\n",
    "metrics_norm = (metrics_subset - metrics_subset.min()) / (metrics_subset.max() - metrics_subset.min())\n",
    "\n",
    "# Number of variables\n",
    "num_vars = len(metrics_norm.columns)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Colors for each algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each algorithm\n",
    "for algo, color in zip(algorithms, colors):\n",
    "    metrics_values = metrics_norm.loc[algo].values.tolist()\n",
    "    metrics_values += metrics_values[:1]  # Close the circle\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(angles, metrics_values, linewidth=2, linestyle='solid', color=color, label=algo)\n",
    "\n",
    "    # Fill area\n",
    "    ax.fill(angles, metrics_values, color=color, alpha=0.25)\n",
    "\n",
    "# Customize\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_norm.columns.tolist(), fontsize=10)\n",
    "\n",
    "# Legend with adjusted position\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.20), fontsize=10, title=\"Algorithms\")\n",
    "\n",
    "# Title\n",
    "plt.title('Normalized Performance Comparison Across Algorithms', fontsize=12, y=1.10)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
